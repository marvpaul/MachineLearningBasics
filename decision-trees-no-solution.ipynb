{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees (for Classification)\n",
    "\n",
    "In a decision tree the data is split at each node according to a decision rule. This corresponds to nested _if-then-else_-rules. In the _if_-part of such a rule are decison is made based on a feature of the data record. \n",
    "\n",
    "We will use the scikit learn implementation. For this implementation the features must be binary or have (at least) ordinal characteristic. If a feature is e.g. nominal with many values, it must be converted to a set of binary (one-hot-coded) features.\n",
    "\n",
    "\n",
    "The splitting rules in the scikit learn implementation are binary and are based on a threshold, e.g.\n",
    "  - _if $x_6 <= 2.14$ then_ left subbranch, _else_ right subbranch.         \n",
    "  - binary features must be coded as 0/1, so the rule becomes: if $x_2 <= 0.5$ _then_ left subbranch, _else_ right subbranch. \n",
    "\n",
    "\n",
    "<!--The structure of the tree will be determined by data (see below) and a training procedure.-->\n",
    "\n",
    "In the leafs of the tree the (class) predictions are made. There are two possibilities for such an inference: \n",
    "   - hard assignment: Predict for the data records which end up on a leaf by the majority class of the training data that end up on that leaf.          \n",
    "   - soft assignment: Assign probabilities according to the distribution of the classes in respect to the training data which end up on that leaf.\n",
    "\n",
    "As an example of a decision tree we will learn the following tree from the titanic data set: \n",
    "<img src=\"./data/titanic.png\" width=\"1000px\"/>\n",
    "\n",
    "A full explanation of the tree will be given later. Here just look at the desion rules (first line of the inner nodes) and at the last line of the leafs. In each leaf you see a array with counts of the different targets (train data): [number_died number_survivors] .\n",
    "\n",
    "### Learning \n",
    "\n",
    "Finding a tree that splits the traing data optimal is np-hard. Therefore often a _greedy_-strategy is used:\n",
    "\n",
    "To build up a decision tree the algorithm starts at the root of the tree. The feature and the threshold \n",
    "that splits the training data best (with respect to the classes) are choosen. In an iterative way the whole tree is build up by such splitting rules. \n",
    "\n",
    "There are different criteria for measuring the \"separation (split) quality\". The most important ones are:\n",
    "\n",
    "- Gini Impurity \n",
    "- Information Gain \n",
    "\n",
    "In this tutorial we use the information gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Information Gain as splitting criterion\n",
    "\n",
    "The entropy with respect to the target class variable $y$ of a training data set $\\mathcal D$ is defined as:\n",
    "\n",
    "$$\n",
    " H(y, \\mathcal D) = - \\sum_{y \\in \\mathcal Y} p(y|\\mathcal D) \\log_2 p(y|\\mathcal D)\n",
    "$$\n",
    "with the domain of the target values $\\mathcal Y = \\{t_1, t_2,... \\}$.\n",
    "\n",
    "\n",
    "The probabilities are estimated by \n",
    "$$\n",
    "  p(y=t_i, \\mathcal D) = |\\mathcal D^{(y=t_i)}| /|\\mathcal D| \n",
    "$$    \n",
    "\n",
    "\n",
    "with the number of training data $|\\mathcal D|$  and the number of training data $|\\mathcal D^{(y=t_i)}|$ with target label $t_i$: \n",
    "\n",
    "\n",
    "On a node a (binary) split on a feature $x_k$ is made by the split rule $x_k \\leq v$. \n",
    "As result there are two data sets $\\mathcal D_0$ and $\\mathcal D_1$ for the left resp. the right branch.\n",
    "\n",
    "The feature $x_k$ and the split value $v$ are choosen that they maximize the 'reduction of the entropy' measured by the information gain $I$:\n",
    "$$\n",
    "  I(y; x_k) = H(y, \\mathcal D) - H(y|x_k) = H(y, \\mathcal D) - \\sum_{j=0}^1 p_jH(y, \\mathcal D_j) =\n",
    "  H(y, \\mathcal D) + \\sum_{j=0}^1  \\sum_{y \\in \\mathcal Y} \\frac{|\\mathcal D_j|}{|\\mathcal D|} p(y|\\mathcal D_j) \\log_2 p(y|\\mathcal D_j)\n",
    "$$\n",
    "Note that $p_{j=0}$  is the estimated probability that a random data record of $\\mathcal D$ has feature value $x_k \\leq v$ which can be estimated by ${|\\mathcal D_0|}/{|\\mathcal D|}$ (analog for $j=1$).\n",
    "\n",
    "$p(y=t_i|\\mathcal D_0)$ can also be estimated by the fraction of the counts ${|\\mathcal D_0^{(y=t_i)}|}/{|\\mathcal D_0|}$. \n",
    "So the information gain can be computed just with counts:\n",
    "\n",
    "\n",
    "$$\n",
    "  I(y; x_k) = \n",
    "   \\sum_{y \\in \\mathcal Y} \\frac{|\\mathcal D^{(y=t_i)}|}{|\\mathcal D|}  \\log_2 \\frac{|\\mathcal D^{(y=t_i)}|}{|\\mathcal D|} + \\sum_{j=0}^1  \\sum_{y \\in \\mathcal Y} \\frac{|\\mathcal D_j^{(y=t_i)}|}{|\\mathcal D|}  \\log_2 \\frac{|\\mathcal D_j^{(y=t_i)}|}{|\\mathcal D_j|}\n",
    "$$\n",
    "\n",
    "\n",
    "<!--$|\\mathcal D_0|$ respectivly $|\\mathcal D_1|$ is the number of elements in the splitted data sets.-->\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Overfitting\n",
    "\n",
    "Deep decision trees generalize often poorly. The following remedies reduce overfitting: \n",
    "\n",
    "- Limitation of the maximal depth of the tree. \n",
    "- Pruning with an validation set either during training (pre-pruning) or after training (post-pruning).\n",
    "- Dimensionality reduction (reducing the number of features before training)\n",
    "\n",
    "\n",
    "Also often combining decision trees to an ensemble (decision forests) is used against overfitting.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Survival of the Titanic \n",
    "\n",
    "\n",
    "First you have read in the titanic data with pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "train_df = df = pd.read_csv('/Users/marvinkruger/wissensrepr/dec_tree/train.csv') # Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`train_df` is a [_pandas_](http://pandas.pydata.org/) [data frame](http://pandas.pydata.org/pandas-docs/stable/dsintro.html). \n",
    "Let's view the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PassengerId  Survived  Pclass  \\\n0              1         0       3   \n1              2         1       1   \n2              3         1       3   \n3              4         1       1   \n4              5         0       3   \n5              6         0       3   \n6              7         0       1   \n7              8         0       3   \n8              9         1       3   \n9             10         1       2   \n10            11         1       3   \n11            12         1       1   \n12            13         0       3   \n13            14         0       3   \n14            15         0       3   \n15            16         1       2   \n16            17         0       3   \n17            18         1       2   \n18            19         0       3   \n19            20         1       3   \n20            21         0       2   \n21            22         1       2   \n22            23         1       3   \n23            24         1       1   \n24            25         0       3   \n25            26         1       3   \n26            27         0       3   \n27            28         0       1   \n28            29         1       3   \n29            30         0       3   \n..           ...       ...     ...   \n861          862         0       2   \n862          863         1       1   \n863          864         0       3   \n864          865         0       2   \n865          866         1       2   \n866          867         1       2   \n867          868         0       1   \n868          869         0       3   \n869          870         1       3   \n870          871         0       3   \n871          872         1       1   \n872          873         0       1   \n873          874         0       3   \n874          875         1       2   \n875          876         1       3   \n876          877         0       3   \n877          878         0       3   \n878          879         0       3   \n879          880         1       1   \n880          881         1       2   \n881          882         0       3   \n882          883         0       3   \n883          884         0       2   \n884          885         0       3   \n885          886         0       3   \n886          887         0       2   \n887          888         1       1   \n888          889         0       3   \n889          890         1       1   \n890          891         0       3   \n\n                                                  Name     Sex   Age  SibSp  \\\n0                              Braund, Mr. Owen Harris    male  22.0      1   \n1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n2                               Heikkinen, Miss. Laina  female  26.0      0   \n3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n4                             Allen, Mr. William Henry    male  35.0      0   \n5                                     Moran, Mr. James    male   NaN      0   \n6                              McCarthy, Mr. Timothy J    male  54.0      0   \n7                       Palsson, Master. Gosta Leonard    male   2.0      3   \n8    Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)  female  27.0      0   \n9                  Nasser, Mrs. Nicholas (Adele Achem)  female  14.0      1   \n10                     Sandstrom, Miss. Marguerite Rut  female   4.0      1   \n11                            Bonnell, Miss. Elizabeth  female  58.0      0   \n12                      Saundercock, Mr. William Henry    male  20.0      0   \n13                         Andersson, Mr. Anders Johan    male  39.0      1   \n14                Vestrom, Miss. Hulda Amanda Adolfina  female  14.0      0   \n15                    Hewlett, Mrs. (Mary D Kingcome)   female  55.0      0   \n16                                Rice, Master. Eugene    male   2.0      4   \n17                        Williams, Mr. Charles Eugene    male   NaN      0   \n18   Vander Planke, Mrs. Julius (Emelia Maria Vande...  female  31.0      1   \n19                             Masselmani, Mrs. Fatima  female   NaN      0   \n20                                Fynney, Mr. Joseph J    male  35.0      0   \n21                               Beesley, Mr. Lawrence    male  34.0      0   \n22                         McGowan, Miss. Anna \"Annie\"  female  15.0      0   \n23                        Sloper, Mr. William Thompson    male  28.0      0   \n24                       Palsson, Miss. Torborg Danira  female   8.0      3   \n25   Asplund, Mrs. Carl Oscar (Selma Augusta Emilia...  female  38.0      1   \n26                             Emir, Mr. Farred Chehab    male   NaN      0   \n27                      Fortune, Mr. Charles Alexander    male  19.0      3   \n28                       O'Dwyer, Miss. Ellen \"Nellie\"  female   NaN      0   \n29                                 Todoroff, Mr. Lalio    male   NaN      0   \n..                                                 ...     ...   ...    ...   \n861                        Giles, Mr. Frederick Edward    male  21.0      1   \n862  Swift, Mrs. Frederick Joel (Margaret Welles Ba...  female  48.0      0   \n863                  Sage, Miss. Dorothy Edith \"Dolly\"  female   NaN      8   \n864                             Gill, Mr. John William    male  24.0      0   \n865                           Bystrom, Mrs. (Karolina)  female  42.0      0   \n866                       Duran y More, Miss. Asuncion  female  27.0      1   \n867               Roebling, Mr. Washington Augustus II    male  31.0      0   \n868                        van Melkebeke, Mr. Philemon    male   NaN      0   \n869                    Johnson, Master. Harold Theodor    male   4.0      1   \n870                                  Balkic, Mr. Cerin    male  26.0      0   \n871   Beckwith, Mrs. Richard Leonard (Sallie Monypeny)  female  47.0      1   \n872                           Carlsson, Mr. Frans Olof    male  33.0      0   \n873                        Vander Cruyssen, Mr. Victor    male  47.0      0   \n874              Abelson, Mrs. Samuel (Hannah Wizosky)  female  28.0      1   \n875                   Najib, Miss. Adele Kiamie \"Jane\"  female  15.0      0   \n876                      Gustafsson, Mr. Alfred Ossian    male  20.0      0   \n877                               Petroff, Mr. Nedelio    male  19.0      0   \n878                                 Laleff, Mr. Kristo    male   NaN      0   \n879      Potter, Mrs. Thomas Jr (Lily Alexenia Wilson)  female  56.0      0   \n880       Shelley, Mrs. William (Imanita Parrish Hall)  female  25.0      0   \n881                                 Markun, Mr. Johann    male  33.0      0   \n882                       Dahlberg, Miss. Gerda Ulrika  female  22.0      0   \n883                      Banfield, Mr. Frederick James    male  28.0      0   \n884                             Sutehall, Mr. Henry Jr    male  25.0      0   \n885               Rice, Mrs. William (Margaret Norton)  female  39.0      0   \n886                              Montvila, Rev. Juozas    male  27.0      0   \n887                       Graham, Miss. Margaret Edith  female  19.0      0   \n888           Johnston, Miss. Catherine Helen \"Carrie\"  female   NaN      1   \n889                              Behr, Mr. Karl Howell    male  26.0      0   \n890                                Dooley, Mr. Patrick    male  32.0      0   \n\n     Parch            Ticket      Fare        Cabin Embarked  \n0        0         A/5 21171    7.2500          NaN        S  \n1        0          PC 17599   71.2833          C85        C  \n2        0  STON/O2. 3101282    7.9250          NaN        S  \n3        0            113803   53.1000         C123        S  \n4        0            373450    8.0500          NaN        S  \n5        0            330877    8.4583          NaN        Q  \n6        0             17463   51.8625          E46        S  \n7        1            349909   21.0750          NaN        S  \n8        2            347742   11.1333          NaN        S  \n9        0            237736   30.0708          NaN        C  \n10       1           PP 9549   16.7000           G6        S  \n11       0            113783   26.5500         C103        S  \n12       0         A/5. 2151    8.0500          NaN        S  \n13       5            347082   31.2750          NaN        S  \n14       0            350406    7.8542          NaN        S  \n15       0            248706   16.0000          NaN        S  \n16       1            382652   29.1250          NaN        Q  \n17       0            244373   13.0000          NaN        S  \n18       0            345763   18.0000          NaN        S  \n19       0              2649    7.2250          NaN        C  \n20       0            239865   26.0000          NaN        S  \n21       0            248698   13.0000          D56        S  \n22       0            330923    8.0292          NaN        Q  \n23       0            113788   35.5000           A6        S  \n24       1            349909   21.0750          NaN        S  \n25       5            347077   31.3875          NaN        S  \n26       0              2631    7.2250          NaN        C  \n27       2             19950  263.0000  C23 C25 C27        S  \n28       0            330959    7.8792          NaN        Q  \n29       0            349216    7.8958          NaN        S  \n..     ...               ...       ...          ...      ...  \n861      0             28134   11.5000          NaN        S  \n862      0             17466   25.9292          D17        S  \n863      2          CA. 2343   69.5500          NaN        S  \n864      0            233866   13.0000          NaN        S  \n865      0            236852   13.0000          NaN        S  \n866      0     SC/PARIS 2149   13.8583          NaN        C  \n867      0          PC 17590   50.4958          A24        S  \n868      0            345777    9.5000          NaN        S  \n869      1            347742   11.1333          NaN        S  \n870      0            349248    7.8958          NaN        S  \n871      1             11751   52.5542          D35        S  \n872      0               695    5.0000  B51 B53 B55        S  \n873      0            345765    9.0000          NaN        S  \n874      0         P/PP 3381   24.0000          NaN        C  \n875      0              2667    7.2250          NaN        C  \n876      0              7534    9.8458          NaN        S  \n877      0            349212    7.8958          NaN        S  \n878      0            349217    7.8958          NaN        S  \n879      1             11767   83.1583          C50        C  \n880      1            230433   26.0000          NaN        S  \n881      0            349257    7.8958          NaN        S  \n882      0              7552   10.5167          NaN        S  \n883      0  C.A./SOTON 34068   10.5000          NaN        S  \n884      0   SOTON/OQ 392076    7.0500          NaN        S  \n885      5            382652   29.1250          NaN        Q  \n886      0            211536   13.0000          NaN        S  \n887      0            112053   30.0000          B42        S  \n888      2        W./C. 6607   23.4500          NaN        S  \n889      0            111369   30.0000         C148        C  \n890      0            370376    7.7500          NaN        Q  \n\n[891 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Scikit's learn decision trees can handle only numeric data. So we must convert the nominal `Sex` feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_df[\"Sex\"] = df.Sex.astype(\"category\").cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`Survived` is the target, that we want to predict from the values of the other columns.   \n",
    "But not all of the other columns are helpful for classification. So we choose a feature set by hand and convert the features into a numpy array for scikit learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "y = targets = df[\"Survived\"]# =  TODO as Exercise\n",
    "columns = [\"Fare\", \"Pclass\", \"Sex\", \"Age\", \"SibSp\"]\n",
    "features = df[columns] #TODO as Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are missing values (`nan`). We use the scikit learn `Imputer` to replace them by the mean of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  7.25     3.       1.      22.       1.    ]\n [ 71.2833   1.       0.      38.       1.    ]\n [  7.925    3.       0.      26.       0.    ]\n ..., \n [ 23.45     3.       0.       6.8625   1.    ]\n [ 30.       1.       1.      26.       0.    ]\n [  7.75     3.       1.      32.       0.    ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "imp=Imputer(missing_values=\"NaN\", strategy=\"mean\", axis=1)\n",
    "filled_df = pd.DataFrame(imp.fit_transform(features))\n",
    "filled_df.columns = features.columns\n",
    "filled_df.index = features.index\n",
    "features = filled_df\n",
    "features = np.array(features)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we are ready to learn a decision tree by the criterion 'Information Gain' and we restrict the depth of the tree to 3.\n",
    "We use the [scikit learn decison tree module](http://scikit-learn.org/stable/modules/tree.html).\n",
    "\n",
    "We use a splitting criterion the 'Information Gain'!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(criterion=\"entropy\", ... # TODO as Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`clf` is an instance of a trained decision tree classifier.\n",
    "\n",
    "The decision tree can be visualized. For this we must write an graphviz dot-File  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.externals.six import StringIO\n",
    "with open(\"data/titanic.dot\", 'w') as f:\n",
    "    f = tree.export_graphviz(clf, out_file=f, feature_names=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The dot file can be converted with the graphiz `dot`- renderer to an image.\n",
    "\n",
    "    dot -Tpng titanic.dot -o titanic.png\n",
    "\n",
    "Here is the graph:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./data/titanic.png\" width=\"1000px\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "According to the decision tree the main criterion (root node) for survival is the sex of the passenger. In the left subtree are the male passengers (sex = 0), in the right subtree the female (sex=1). \n",
    "\n",
    "In the leafs the class information is given by a `value` array. Here the second value is the number of survivers in the leafs.\n",
    "\n",
    "For example the leftmost leaf represents passengers that are male (sex=0) with fare<=26.2687 and age<=13.5. 13 of such boys survived and 2 of them died.\n",
    "\n",
    "The entropy $- \\sum p_i \\log_2 (p_i)$ is displayed also at each node (splitting criterion).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercises: Splitting criterion entropy / information gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise 1: \n",
    "\n",
    "Compute the root node entropy (with numpy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise 2\n",
    "\n",
    "Compute the information gain of the first split node (root node). Use the entropy values and the number of data records (samples)\n",
    "from the decision tree image. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise 3\n",
    "Compute the information gain of the following split table:\n",
    "\n",
    "|  | class 0  | class 1  |  \n",
    "|---|---|---|\n",
    "| feature <= v| 2  | 13  |      \n",
    "| feature > v | 359  |  41 |   \n",
    "\n",
    "The numbers are the corresponding data records, e.g. there are 13 data records with target class 1 and feature <= v. \n",
    "\n",
    "Write a python function that computes the information gain.The data is given by a python array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "data = np.array([[2.,13.],[359., 41.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Prediction\n",
    "\n",
    "To make predictions with scikit learn, we must convert the data in\n",
    "the same way as we did it with the training data. Then we could use the method:    \n",
    "\n",
    "    clf.predict(testdata)\n",
    "    \n",
    "Note: The depth of the decision tree is a hyperparameter. So the depth should be determined with the help of a \n",
    "validation set or by cross validation.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Literature\n",
    "\n",
    "- [Cri] [A. Criminisi, J. Shotton, and E. Konukoglu, Decision Forests for Classification, Regression, Density Estimation, Manifold Learning and Semi- Supervised Learning, no. MSR-TR-2011-114, 28 October 2011.](http://research.microsoft.com/pubs/155552/decisionForests_MSR_TR_2011_114.pdf)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
